import requests
from bs4 import BeautifulSoup
import time
import os
from datetime import datetime

def crawl_naver_news_robust(base_keywords, include_words=None, exclude_words=None, pages=1):
    if isinstance(base_keywords, str): base_keywords = [base_keywords]
    
    # ê¹ƒí—ˆë¸Œ ì•¡ì…˜ ì„œë²„ì„ì„ ìˆ¨ê¸°ê¸° ìœ„í•œ ë” ê°•ë ¥í•œ í—¤ë”
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://www.naver.com/'
    }

    query_parts = [f"({' | '.join(base_keywords)})"]
    if include_words: query_parts.append(" ".join([f"+{word}" for word in include_words]))
    if exclude_words: query_parts.append(" ".join([f"-{word}" for word in exclude_words]))
    query = " ".join(query_parts)

    results = []
    print(f"ğŸ” ê²€ìƒ‰ì–´: [{query}]")

    for page in range(pages):
        start_val = (page * 10) + 1
        params = {
            'where': 'news',
            'query': query,
            'sm': 'tab_opt',
            'sort': 1,
            'pd': 2,
            'nso': 'so:dd,p:2d,a:all',
            'start': start_val
        }

        try:
            response = requests.get("https://search.naver.com/search.naver", headers=headers, params=params, timeout=10)
            
            # [ë””ë²„ê·¸ 1] ì‘ë‹µ ì½”ë“œ í™•ì¸
            if response.status_code != 200:
                print(f"âŒ ë„¤ì´ë²„ ì‘ë‹µ ì—ëŸ¬ (Status Code: {response.status_code})")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # [ë””ë²„ê·¸ 2] ê¸°ì‚¬ ì œëª© íƒœê·¸ ì§ì ‘ ì°¾ê¸° (ì„ íƒì ë‹¨ìˆœí™”)
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ì œëª©ì€ ë³´í†µ 'news_tit' í´ë˜ìŠ¤ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
            all_links = soup.find_all("a", class_="news_tit")
            
            if not all_links:
                # í´ë˜ìŠ¤ë¡œ ëª» ì°¾ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë°±ì—… (ê¸°ì¡´ ë°©ì‹)
                main_pack = soup.select_one("#main_pack")
                all_links = main_pack.find_all("a") if main_pack else []

            print(f"ğŸ“¡ {page+1}í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ì „ì²´ ë§í¬ ìˆ˜: {len(all_links)}")

            found_in_page = 0
            for link in all_links:
                text = link.get_text().strip()
                href = link.get('href')

                # í•„í„°ë§ ì¡°ê±´
                if 10 < len(text) < 100 and href and href.startswith("http"):
                    # ì œì™¸ í‚¤ì›Œë“œ ê²€ì‚¬
                    if exclude_words and any(bad in text for bad in exclude_words): continue
                    
                    # í‚¤ì›Œë“œ í¬í•¨ ê²€ì‚¬
                    if not any(k in text for k in base_keywords): continue
                    
                    # ì¤‘ë³µ ê²€ì‚¬
                    if any(r['url'] == href for r in results): continue

                    results.append({'title': text, 'url': href})
                    found_in_page += 1

            print(f"âœ… {page+1}í˜ì´ì§€ í•„í„°ë§ í†µê³¼ ê¸°ì‚¬: {found_in_page}ê±´")
            if len(all_links) == 0: break
            time.sleep(1)

        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")
            break

    return results

def format_news_report(news_data):
    sector_invest = []
    sector_industry = []

    for item in news_data:
        title = item['title']
        if any(word in title for word in ['ì†ìµ', 'ìì‚°', 'íˆ¬ì', 'ì¬ë¬´']):
            if len(sector_invest) < 5: sector_invest.append(item)
        else:
            if len(sector_industry) < 5: sector_industry.append(item)

    today = datetime.now().strftime("%Y-%m-%d")
    report = f"â– News feed: {today}\n\n"
    
    report += "<ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>\n\n"
    if not sector_industry: report += "(ê¸°ì‚¬ ì—†ìŒ)\n\n"
    for item in sector_industry: report += f"{item['title']}\n{item['url']}\n\n"
        
    report += "<íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>\n\n"
    if not sector_invest: report += "(ê¸°ì‚¬ ì—†ìŒ)\n\n"
    for item in sector_invest: report += f"{item['title']}\n{item['url']}\n\n"
        
    return report

def send_telegram_msg(message):
    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    if not token or not chat_id: return
    try:
        requests.post(f"https://api.telegram.org/bot{token}/sendMessage", 
                      data={'chat_id': chat_id, 'text': message})
    except: pass

if __name__ == "__main__":
    KEYWORDS = ["í•œí™”ìƒëª…", "ì‚¼ì„±ìƒëª…", "êµë³´ìƒëª…", "ìƒë³´ì‚¬", "ë³´í—˜ì‚¬"]
    #EXCLUDES = ["ë°°íƒ€ì ", "ìƒí’ˆ", "ê°„ë³‘", "ì‚¬ì—…ë¹„", "ë³´í—˜ê¸ˆ", "ì—°ê¸ˆë³´í—˜", "ë¯¼ì›"]
    
    news_list = crawl_naver_news_robust(KEYWORDS, exclude_words=EXCLUDES, pages=5)
    print(f"ğŸ“Š ìµœì¢… ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì´í•©: {len(news_list)}ê±´")
    
    report_text = format_news_report(news_list)
    print(report_text)
    send_telegram_msg(report_text)
