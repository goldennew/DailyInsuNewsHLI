import requests
from bs4 import BeautifulSoup
import time
import os
from datetime import datetime

# --- 1. í¬ë¡¤ë§ í•¨ìˆ˜ (ì‚¬ìš©ìê»˜ì„œ ì œê³µí•˜ì‹  ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€) ---
def crawl_naver_news_robust(base_keywords, include_words=None, exclude_words=None, pages=1):
    if isinstance(base_keywords, str): base_keywords = [base_keywords]
    if isinstance(include_words, str): include_words = [include_words]
    if isinstance(exclude_words, str): exclude_words = [exclude_words]

    print(f"ğŸ•’ ì¡°íšŒ ê¸°ì¤€: í˜„ì¬ ì‹œê°„({datetime.now().strftime('%Y-%m-%d %H:%M')})ìœ¼ë¡œë¶€í„° 48ì‹œê°„ ì´ë‚´")

    query_parts = []
    if base_keywords:
        if len(base_keywords) > 1:
            query_parts.append(f"({' | '.join(base_keywords)})")
        else:
            query_parts.append(base_keywords[0])

    if include_words: query_parts.append(" ".join([f"+{word}" for word in include_words]))
    if exclude_words: query_parts.append(" ".join([f"-{word}" for word in exclude_words]))

    query = " ".join(query_parts)
    base_url = "https://search.naver.com/search.naver"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    results = []
    print(f"--- ê²€ìƒ‰ì–´: [{query}] í¬ë¡¤ë§ ì‹œì‘ ---")

    for page in range(pages):
        start_val = (page * 10) + 1
        params = {
            'where': 'news',
            'query': query,
            'sm': 'tab_opt',
            'sort': 1,
            'pd': 2,
            'nso': 'so:dd,p:2d,a:all',
            'start': start_val
        }

        try:
            response = requests.get(base_url, headers=headers, params=params)
            soup = BeautifulSoup(response.text, 'html.parser')
            main_pack = soup.select_one("#main_pack")
            if not main_pack: main_pack = soup.body

            all_links = main_pack.find_all("a")
            found_in_page = 0

            for link in all_links:
                text = link.get_text().strip()
                href = link.get('href')

                # ì‚¬ìš©ì í•„í„°: 10ì ì´ˆê³¼ 100ì ë¯¸ë§Œ
                if 10 < len(text) < 100 and href and href.startswith("http"):
                    is_excluded = False
                    if exclude_words:
                        for bad_word in exclude_words:
                            if bad_word in text:
                                is_excluded = True
                                break
                    if is_excluded: continue

                    matched_keywords = []
                    for k in base_keywords:
                        if k in text:
                            matched_keywords.append(k)

                    if not matched_keywords: continue
                    if any(r['url'] == href for r in results): continue
                    if text in ["ë„¤ì´ë²„ë‰´ìŠ¤", "ê´€ë ¨ë‰´ìŠ¤"]: continue

                    results.append({
                        'title': text,
                        'url': href
                    })
                    found_in_page += 1
                    if found_in_page >= 15: break

            print(f"[{page+1}í˜ì´ì§€] {found_in_page}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            if found_in_page == 0: break
            time.sleep(0.5)
        except Exception as e:
            print(f"ì—ëŸ¬ ë°œìƒ: {e}")
            break

    return results

# --- 2. ë¶„ë¥˜ ë° ì¶œë ¥ í˜•ì‹ ì •ë¦¬ (ìš”êµ¬ì‚¬í•­ ë°˜ì˜) ---
def format_news_report(news_data):
    sector_invest = [] # <íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>
    sector_industry = [] # <ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>

    for item in news_data:
        title = item['title']
        # 'ì†ìµ' ë˜ëŠ” 'ìì‚°'ì´ í¬í•¨ë˜ë©´ íˆ¬ì ì„¹í„°ë¡œ ë¶„ë¥˜
        if 'ì†ìµ' in title or 'ìì‚°' in title:
            if len(sector_invest) < 5:
                sector_invest.append(item)
        else:
            if len(sector_industry) < 5:
                sector_industry.append(item)
        
        # ë‘˜ ë‹¤ 5ê°œì”© ì°¼ìœ¼ë©´ ì¢…ë£Œ
        if len(sector_invest) >= 5 and len(sector_industry) >= 5:
            break

    today = datetime.now().strftime("%Y-%m-%d")
    
    # [ì¶œë ¥ í˜•ì‹ ì—„ê²© ì¤€ìˆ˜]
    report = f"â– News feed: {today}\n"
    
    report += "<ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>\n\n"
    for item in sector_industry:
        report += f"{item['title']}\n{item['url']}\n\n"
        
    report += "<íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>\n\n"
    for item in sector_invest:
        report += f"{item['title']}\n{item['url']}\n\n"
        
    return report

# --- 3. í…”ë ˆê·¸ë¨ ì „ì†¡ í•¨ìˆ˜ ---
def send_telegram_msg(message):
    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    if not token or not chat_id:
        print("âŒ í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    # ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜ (í˜„ì¬ ì„¤ì •ìƒ ì•ˆì „)
    payload = {'chat_id': chat_id, 'text': message}
    
    try:
        res = requests.post(url, data=payload)
        if res.status_code == 200:
            print("âœ… í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ!")
        else:
            print(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {res.text}")
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {e}")

# --- 4. ìµœì¢… ì‹¤í–‰ ---
if __name__ == "__main__":
    KEYWORDS = ["í•œí™”ìƒëª…", "ì‚¼ì„±ìƒëª…", "êµë³´ìƒëª…", "ìƒë³´ì‚¬", "ë³´í—˜ì‚¬"]
    EXCLUDES = ["ë°°íƒ€ì ", "ìƒí’ˆ", "ê°„ë³‘", "ì‚¬ì—…ë¹„", "ë³´í—˜ê¸ˆ", "ì—°ê¸ˆë³´í—˜", "ë¯¼ì›"]
    
    # 1. í¬ë¡¤ë§ (ì‚¬ìš©ì ì½”ë“œ ì‹¤í–‰)
    news_list = crawl_naver_news_robust(KEYWORDS, exclude_words=EXCLUDES, pages=5)
    
    # 2. í˜•ì‹ ì •ë¦¬
    report_text = format_news_report(news_list)
    
    # 3. ì¶œë ¥ ë° ì „ì†¡
    print("\n--- ìƒì„±ëœ ë¦¬í¬íŠ¸ ---")
    print(report_text)
    send_telegram_msg(report_text)
