import requests
from bs4 import BeautifulSoup
import time
import os
from datetime import datetime, timedelta

def parse_naver_time(time_str):
    """ë„¤ì´ë²„ì˜ 'nì‹œê°„ ì „', '1ì¼ ì „' ë“± í…ìŠ¤íŠ¸ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜"""
    now = datetime.now()
    try:
        if 'ë¶„ ì „' in time_str:
            minutes = int(time_str.replace('ë¶„ ì „', '').strip())
            return now - timedelta(minutes=minutes)
        elif 'ì‹œê°„ ì „' in time_str:
            hours = int(time_str.replace('ì‹œê°„ ì „', '').strip())
            return now - timedelta(hours=hours)
        elif 'ì¼ ì „' in time_str:
            days = int(time_str.replace('ì¼ ì „', '').strip())
            return now - timedelta(days=days)
        elif '.' in time_str: # ì˜ˆ: 2026.02.04.
            return datetime.strptime(time_str.strip('. '), '%Y.%m.%d')
        return now # ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ë°˜í™˜
    except:
        return now

def crawl_naver_news_robust(keywords, pages=3):
    base_url = "https://m.search.naver.com/search.naver"
    headers = {
        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
        'Accept-Language': 'ko-kr',
        'Referer': 'https://m.naver.com/'
    }

    results = []
    query = " ".join(keywords)
    
    # --- í•„í„° ê¸°ì¤€ ì„¤ì • (í˜„ì¬ë¡œë¶€í„° 36ì‹œê°„ ì „) ---
    limit_time = datetime.now() - timedelta(hours=36)
    print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: {query}")
    print(f"â° í•„í„° ê¸°ì¤€: {limit_time.strftime('%Y-%m-%d %H:%M')} ì´í›„ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘")

    for page in range(pages):
        start = (page * 15) + 1
        params = {
            'where': 'm_news',
            'query': query,
            'sm': 'mtb_opt',
            'sort': '1', # ìµœì‹ ìˆœ
            'nso': 'so:dd,p:2d', # ë„¤ì´ë²„ ì˜µì…˜ì€ 2ì¼ë¡œ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
            'start': start
        }

        try:
            response = requests.get(base_url, headers=headers, params=params, timeout=15)
            if response.status_code != 200: continue

            soup = BeautifulSoup(response.text, 'html.parser')
            news_items = soup.select("div.news_wrap") or soup.select("li.bx")

            found_in_page = 0
            for item in news_items:
                title_tag = item.select_one("a.news_tit") or item.select_one("div.api_txt_lines.tit")
                time_tag = item.select_one("span.sub_txt") # ì‹œê°„ ì •ë³´ê°€ ë‹´ê¸´ íƒœê·¸
                
                if not title_tag: continue
                
                text = title_tag.get_text().strip()
                href = title_tag.get('href') if title_tag.has_attr('href') else title_tag.parent.get('href')
                raw_time = time_tag.get_text().strip() if time_tag else "ì•Œ ìˆ˜ ì—†ìŒ"

                # 1. ì‹œê°„ í•„í„°ë§ (36ì‹œê°„ ì´ë‚´ ì—¬ë¶€)
                article_time = parse_naver_time(raw_time)
                if article_time < limit_time:
                    # 36ì‹œê°„ë³´ë‹¤ ì˜¤ë˜ëœ ê¸°ì‚¬ë¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
                    continue

                # 2. ì œëª© ê¸¸ì´ í•„í„° (10~100ì)
                if 10 < len(text) < 100 and href and href.startswith("http"):
                    if any(r['url'] == href for r in results): continue
                    
                    results.append({'title': text, 'url': href, 'time': raw_time})
                    found_in_page += 1

            print(f"ğŸ“„ {page+1}í˜ì´ì§€: {found_in_page}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            if found_in_page == 0 and page > 0: break # ë” ì´ìƒ ìµœì‹  ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            
            time.sleep(1.0)

        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")
            break

    return results

def format_news_report(news_data):
    sector_invest = []   # <íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>
    sector_industry = [] # <ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>

    for item in news_data:
        title = item['title']
        if 'ì†ìµ' in title or 'ìì‚°' in title:
            if len(sector_invest) < 5: sector_invest.append(item)
        else:
            if len(sector_industry) < 5: sector_industry.append(item)
    
    today = datetime.now().strftime("%Y-%m-%d")
    report = f"â– News feed: {today}\n"
    
    report += "\n<ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>\n\n"
    if not sector_industry: report += "(36ì‹œê°„ ì´ë‚´ ê´€ë ¨ ê¸°ì‚¬ ì—†ìŒ)\n\n"
    for item in sector_industry:
        report += f"{item['title']}\n{item['url']}\n\n"
        
    report += "<íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>\n\n"
    if not sector_invest: report += "(36ì‹œê°„ ì´ë‚´ ê´€ë ¨ ê¸°ì‚¬ ì—†ìŒ)\n\n"
    for item in sector_invest:
        report += f"{item['title']}\n{item['url']}\n\n"
        
    return report

def send_telegram(message):
    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    if not token or not chat_id: return
    try:
        # ë©”ì‹œì§€ê°€ ë„ˆë¬´ ê¸¸ë©´ í…”ë ˆê·¸ë¨ì—ì„œ ê±°ë¶€í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ 4000ìë¡œ ìë¦„
        requests.post(f"https://api.telegram.org/bot{token}/sendMessage", 
                      data={'chat_id': chat_id, 'text': message[:4000], 'disable_web_page_preview': True})
    except: pass

if __name__ == "__main__":
    KEYWORDS = ["ì‚¼ì„±ìƒëª…", "í•œí™”ìƒëª…", "êµë³´ìƒëª…", "ë³´í—˜ì‚¬"]
    news_list = crawl_naver_news_robust(KEYWORDS, pages=3)
    final_msg = format_news_report(news_list)
    
    print(final_msg)
    send_telegram(final_msg)
