import requests
from bs4 import BeautifulSoup
import time
import os
from datetime import datetime

# --- 1. í¬ë¡¤ë§ í•¨ìˆ˜ (ì œê³µí•´ì£¼ì‹  ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€) ---
def crawl_naver_news_robust(base_keywords, include_words=None, exclude_words=None, pages=1):
    if isinstance(base_keywords, str): base_keywords = [base_keywords]
    if isinstance(include_words, str): include_words = [include_words]
    if isinstance(exclude_words, str): exclude_words = [exclude_words]

    print(f"ğŸ•’ ì¡°íšŒ ê¸°ì¤€: í˜„ì¬ ì‹œê°„({datetime.now().strftime('%Y-%m-%d %H:%M')})ìœ¼ë¡œë¶€í„° 48ì‹œê°„ ì´ë‚´")

    query_parts = []
    if base_keywords:
        if len(base_keywords) > 1:
            query_parts.append(f"({' | '.join(base_keywords)})")
        else:
            query_parts.append(base_keywords[0])

    if include_words: query_parts.append(" ".join([f"+{word}" for word in include_words]))
    if exclude_words: query_parts.append(" ".join([f"-{word}" for word in exclude_words]))

    query = " ".join(query_parts)
    base_url = "https://search.naver.com/search.naver"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    results = []
    print(f"--- ê²€ìƒ‰ì–´: [{query}] í¬ë¡¤ë§ ì‹œì‘ ---")

    for page in range(pages):
        start_val = (page * 10) + 1
        params = {
            'where': 'news',
            'query': query,
            'sm': 'tab_opt',
            'sort': 1,
            'pd': 2,
            'nso': 'so:dd,p:2d,a:all',
            'start': start_val
        }

        try:
            response = requests.get(base_url, headers=headers, params=params)
            soup = BeautifulSoup(response.text, 'html.parser')
            main_pack = soup.select_one("#main_pack")
            if not main_pack: main_pack = soup.body

            all_links = main_pack.find_all("a")
            found_in_page = 0

            for link in all_links:
                text = link.get_text().strip()
                href = link.get('href')

                # í•„í„°: ì œëª© ê¸¸ì´ 10ì ì´ìƒ 200ì ì´í•˜ (ìš”ì²­ì‚¬í•­ ë°˜ì˜)
                if 10 <= len(text) <= 200 and href and href.startswith("http"):
                    if exclude_words and any(bad_word in text for bad_word in exclude_words):
                        continue

                    if not any(k in text for k in base_keywords):
                        continue

                    if any(r['url'] == href for r in results): continue
                    if text in ["ë„¤ì´ë²„ë‰´ìŠ¤", "ê´€ë ¨ë‰´ìŠ¤"]: continue

                    results.append({'title': text, 'url': href})
                    found_in_page += 1
                    if found_in_page >= 15: break

            print(f"[{page+1}í˜ì´ì§€] {found_in_page}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            if found_in_page == 0: break
            time.sleep(0.5)
        except Exception as e:
            print(f"ì—ëŸ¬ ë°œìƒ: {e}")
            break

    return results

# --- 2. ë¶„ë¥˜ ë° í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í˜•ì‹í™” (ì¶”ê°€ëœ ë¶€ë¶„) ---
def format_and_classify(news_data):
    sector_invest = []  # <íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>
    sector_industry = [] # <ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>

    for item in news_data:
        title = item['title']
        # 'ì†ìµ' ë˜ëŠ” 'ìì‚°' í¬í•¨ ì—¬ë¶€ë¡œ ë¶„ë¥˜
        if 'ì†ìµ' in title or 'ìì‚°' in title:
            if len(sector_invest) < 5:
                sector_invest.append(item)
        else:
            if len(sector_industry) < 5:
                sector_industry.append(item)
        
        if len(sector_invest) >= 5 and len(sector_industry) >= 5:
            break

    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    
    # ë©”ì‹œì§€ ìƒì„±
    msg = f"â– News feed: {today}\n"
    
    msg += "<ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>\n\n"
    if not sector_industry: msg += "(í•´ë‹¹ ê¸°ì‚¬ ì—†ìŒ)\n\n"
    for item in sector_industry:
        msg += f"{item['title']}\n{item['url']}\n\n"
        
    msg += "<íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>\n\n"
    if not sector_invest: msg += "(í•´ë‹¹ ê¸°ì‚¬ ì—†ìŒ)\n\n"
    for item in sector_invest:
        msg += f"{item['title']}\n{item['url']}\n\n"
        
    return msg

# --- 3. í…”ë ˆê·¸ë¨ ì „ì†¡ í•¨ìˆ˜ (ì¶”ê°€ëœ ë¶€ë¶„) ---
def send_telegram(message):
    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    if not token or not chat_id:
        print("âŒ í…”ë ˆê·¸ë¨ í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸ í•„ìš”")
        return

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {'chat_id': chat_id, 'text': message}
    
    try:
        res = requests.post(url, data=payload)
        if res.status_code == 200: print("âœ… í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ")
        else: print(f"âŒ ì „ì†¡ ì‹¤íŒ¨: {res.text}")
    except Exception as e:
        print(f"âŒ ì „ì†¡ ì—ëŸ¬: {e}")

# --- 4. ì‹¤í–‰ë¶€ ---
if __name__ == "__main__":
    KEYWORDS = ["í•œí™”ìƒëª…", "ì‚¼ì„±ìƒëª…", "êµë³´ìƒëª…", "ìƒë³´ì‚¬", "ë³´í—˜ì‚¬"]
    EXCLUDES = ["ë°°íƒ€ì ", "ìƒí’ˆ", "ê°„ë³‘", "ì‚¬ì—…ë¹„", "ë³´í—˜ê¸ˆ", "ì—°ê¸ˆë³´í—˜", "ë¯¼ì›"]
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    news_data = crawl_naver_news_robust(KEYWORDS, exclude_words=EXCLUDES, pages=5)
    
    # ë¶„ë¥˜ ë° ë©”ì‹œì§€ ìƒì„±
    final_report = format_and_classify(news_data)
    
    # ê²°ê³¼ ì¶œë ¥ ë° í…”ë ˆê·¸ë¨ ì „ì†¡
    print(final_report)
    send_telegram(final_report)
