import requests
from bs4 import BeautifulSoup
import time
import os
from datetime import datetime
import random
from urllib.parse import quote

def crawl_naver_news_pc(keywords, pages=2):
    session = requests.Session()
    
    # 1. ì¼ë°˜ì ì¸ ìœˆë„ìš° PC í¬ë¡¬ ë¸Œë¼ìš°ì €ë¡œ ìœ„ì¥
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }

    # 2. ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸í•˜ì—¬ ì¿ í‚¤ íšë“ (PC ë²„ì „)
    try:
        session.get("https://www.naver.com", headers=headers, timeout=10)
        time.sleep(random.uniform(1.0, 2.0)) # ì‚¬ëŒì´ ì ‘ì†í•œ ì²™ ëœ¸ ë“¤ì´ê¸°
    except:
        pass

    results = []
    # ê²€ìƒ‰ì–´ë¥¼ URL ì¸ì½”ë”© (í•„ìˆ˜)
    query_str = " ".join(keywords)
    
    print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤(PC) ê²€ìƒ‰ ì‹œì‘: {query_str}")

    # PC ë²„ì „ ë‰´ìŠ¤ ê²€ìƒ‰ URL
    base_url = "https://search.naver.com/search.naver"

    for page in range(pages):
        start = (page * 10) + 1  # PCëŠ” í˜ì´ì§€ë‹¹ 10ê°œì”© ë³´ì—¬ì¤ë‹ˆë‹¤.
        
        params = {
            'where': 'news',
            'query': query_str,
            'sort': '1',       # ìµœì‹ ìˆœ
            'nso': 'so:dd,p:2d', # ìµœê·¼ 2ì¼
            'start': start
        }

        try:
            # ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•œ ëœë¤ ì§€ì—°
            time.sleep(random.uniform(2.0, 4.0))
            
            response = session.get(base_url, headers=headers, params=params, timeout=15)
            
            if response.status_code != 200:
                print(f"âŒ ì ‘ì† ì‹¤íŒ¨ (ì½”ë“œ: {response.status_code})")
                continue

            # ì°¨ë‹¨ ì—¬ë¶€ í™•ì¸
            if "captcha" in response.url or "ë¡œë´‡" in response.text:
                print("ğŸš¨ ë„¤ì´ë²„ê°€ í˜„ì¬ IPë¥¼ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤. (VPNì„ ë„ê±°ë‚˜ ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”)")
                break

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # PC ë²„ì „ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì„ íƒì
            # êµ¬ì¡°: ul.list_news > li.bx > div.news_wrap
            news_items = soup.select("div.news_wrap")

            if not news_items:
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ
                print(f"â„¹ï¸ {page+1}í˜ì´ì§€: ê¸°ì‚¬ ì—†ìŒ")
                break

            found_in_page = 0
            for item in news_items:
                # ì œëª© íƒœê·¸ (PC ë²„ì „: a.news_tit)
                title_tag = item.select_one("a.news_tit")
                if not title_tag: continue
                
                text = title_tag.get_text().strip()
                href = title_tag.get('href')

                # í•„í„°ë§
                if 5 < len(text) < 120 and href and href.startswith("http"):
                    # ì¤‘ë³µ ì œê±°
                    if any(r['url'] == href for r in results): continue
                    
                    results.append({'title': text, 'url': href})
                    found_in_page += 1

            print(f"ğŸ“„ {page+1}í˜ì´ì§€: {found_in_page}ê±´ ìˆ˜ì§‘")
            
            if found_in_page == 0:
                break

        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")
            break

    return results

# ë¦¬í¬íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´ ë‹¤ì‹œ í¬í•¨)
def format_news_report(news_data):
    sector_invest = []   
    sector_industry = [] 

    for item in news_data:
        title = item['title']
        if any(keyword in title for keyword in ['ì†ìµ', 'ìì‚°', 'ê¸ˆìœµ', 'ì‹œì¥', 'íˆ¬ì', 'ê¸ˆë¦¬']):
            if len(sector_invest) < 5: sector_invest.append(item)
        else:
            if len(sector_industry) < 5: sector_industry.append(item)
    
    today = datetime.now().strftime("%Y-%m-%d")
    report = f"â–  News feed: {today}\n"
    
    report += "\n<ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>\n"
    if not sector_industry: report += "(ê¸°ì‚¬ ì—†ìŒ)\n"
    for item in sector_industry:
        report += f"â€¢ {item['title']}\n{item['url']}\n\n"
        
    report += "<íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>\n"
    if not sector_invest: report += "(ê¸°ì‚¬ ì—†ìŒ)\n"
    for item in sector_invest:
        report += f"â€¢ {item['title']}\n{item['url']}\n\n"
        
    return report

def send_telegram(message):
    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    if not token or not chat_id: return
    try:
        requests.post(f"https://api.telegram.org/bot{token}/sendMessage", 
                      data={'chat_id': chat_id, 'text': message, 'disable_web_page_preview': True})
    except: pass

if __name__ == "__main__":
    KEYWORDS = ["ì‚¼ì„±ìƒëª…", "í•œí™”ìƒëª…", "êµë³´ìƒëª…"]
    
    # í•¨ìˆ˜ ì´ë¦„ ë³€ê²½ë¨ (PC ë²„ì „)
    news_list = crawl_naver_news_pc(KEYWORDS, pages=2)
    
    final_msg = format_news_report(news_list)
    print(final_msg)
    send_telegram(final_msg)
