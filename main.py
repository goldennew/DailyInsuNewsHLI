import requests
import os
import html
import difflib
import time
from datetime import datetime

# ==========================================
# ğŸ”‘ API í‚¤ ì„¤ì •
# ==========================================
NAVER_CLIENT_ID = "2cC4xeZPfKKs3BVY_onT"
NAVER_CLIENT_SECRET = "21DmUYrAdX"

if os.environ.get("NAVER_CLIENT_ID"):
    NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
    NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")

def crawl_naver_news_api(target_keywords, excludes=[], display_limit=50):
    """
    íŠ¹ì • í‚¤ì›Œë“œ ê·¸ë£¹ì— ëŒ€í•´ì„œë§Œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    """
    url = "https://openapi.naver.com/v1/search/news.json"
    
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    
    # í•´ë‹¹ ê·¸ë£¹ì˜ í‚¤ì›Œë“œë¡œ ì¿¼ë¦¬ ìƒì„±
    query = " | ".join(target_keywords)
    print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: [{query}] (ìš”ì²­ {display_limit}ê±´)")

    results = []
    
    # API í˜¸ì¶œ íšŸìˆ˜ ê³„ì‚° (1íšŒ ìµœëŒ€ 100ê°œ)
    loop_count = (display_limit // 100) + 1 if display_limit % 100 != 0 else (display_limit // 100)
    
    for i in range(loop_count):
        req_display = 100 if display_limit > 100 else display_limit
        display_limit -= req_display
        
        start = (i * 100) + 1
        
        params = {
            "query": query,
            "display": req_display,
            "start": start,
            "sort": "date"
        }

        try:
            response = requests.get(url, headers=headers, params=params)
            if response.status_code != 200:
                print(f"âŒ API í˜¸ì¶œ ì—ëŸ¬: {response.status_code}")
                break

            items = response.json().get('items', [])
            if not items: break

            for item in items:
                raw_title = item['title']
                clean_title = html.unescape(raw_title).replace("<b>", "").replace("</b>", "")
                
                raw_desc = item['description']
                clean_desc = html.unescape(raw_desc).replace("<b>", "").replace("</b>", "")
                
                link = item['originallink'] if item['originallink'] else item['link']

                # 1. ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
                if any(ex_word in clean_title for ex_word in excludes):
                    continue

                # 2. í•„ìˆ˜ í‚¤ì›Œë“œ ì²´í¬ (ì œëª© ê¸°ì¤€)
                if not any(key_word in clean_title for key_word in target_keywords):
                    continue
                
                results.append({'title': clean_title, 'url': link, 'desc': clean_desc})
            
            time.sleep(0.3) 

        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬: {e}")
            break
            
    print(f"   ğŸ‘‰ ìˆ˜ì§‘ ì™„ë£Œ: {len(results)}ê±´")
    return results

def remove_duplicates_globally(all_news):
    """
    í•©ì³ì§„ ì „ì²´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µ(URL ë° ë‚´ìš©)ì„ ì œê±°
    """
    unique_news = []
    seen_urls = set()
    seen_descriptions = []

    print("ğŸ§¹ ì „ì²´ ì¤‘ë³µ ì œê±° ë° ì •ì œ ì‘ì—… ì¤‘...")

    for item in all_news:
        # URL ì¤‘ë³µ ì²´í¬
        if item['url'] in seen_urls:
            continue
            
        # ë³¸ë¬¸ ë‚´ìš© ìœ ì‚¬ë„ ì²´í¬ (30ì ì´ìƒ ê²¹ì¹˜ë©´ ì¤‘ë³µ ì²˜ë¦¬)
        is_content_dup = False
        for exist_desc in seen_descriptions:
            matcher = difflib.SequenceMatcher(None, item['desc'], exist_desc)
            match = matcher.find_longest_match(0, len(item['desc']), 0, len(exist_desc))
            
            if match.size >= 10: 
                is_content_dup = True
