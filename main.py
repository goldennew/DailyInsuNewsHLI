import requests
import os
import html
import difflib
import time
import sys
from datetime import datetime, timedelta

# ==========================================
# ğŸ”‘ API í‚¤ ì„¤ì •
# ==========================================
NAVER_CLIENT_ID = "2cC4xeZPfKKs3BVY_onT"
NAVER_CLIENT_SECRET = "21DmUYrAdX"

if os.environ.get("NAVER_CLIENT_ID"):
    NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
    NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")

# ==========================================
# ğŸ“… íœ´ì¼ ì²´í¬ í•¨ìˆ˜
# ==========================================
def is_skip_day():
    now_date = datetime.now().date()
    
    if now_date.weekday() >= 5:
        return True, "ì£¼ë§(í† /ì¼)"

    holidays = [
        "2026-01-01", 
        "2026-02-17", "2026-02-18", "2026-02-19",
        "2026-03-01", "2026-03-02",
        "2026-05-05", "2026-05-24", "2026-05-25", 
        "2026-06-06", "2026-08-15", 
        "2026-09-24", "2026-09-25", "2026-09-26",
        "2026-10-03", "2026-10-09", "2026-12-25"
    ]
    
    if str(now_date) in holidays:
        return True, "ì§€ì • ê³µíœ´ì¼"

    return False, ""

def crawl_naver_news_api(target_keywords, excludes=[], display_limit=50, category_tag='general'):
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    
    query = " | ".join(target_keywords)
    print(f"ğŸ” [{category_tag}] ê²€ìƒ‰ ì‹œì‘: {query} (ìš”ì²­ {display_limit}ê±´)")

    results = []
    loop_count = (display_limit // 100) + 1 if display_limit % 100 != 0 else (display_limit // 100)
    
    for i in range(loop_count):
        req_display = 100 if display_limit > 100 else display_limit
        display_limit -= req_display
        start = (i * 100) + 1
        
        params = {"query": query, "display": req_display, "start": start, "sort": "date"}

        try:
            response = requests.get(url, headers=headers, params=params)
            if response.status_code != 200:
                print(f"âŒ API í˜¸ì¶œ ì—ëŸ¬: {response.status_code}")
                break

            items = response.json().get('items', [])
            if not items: break

            for item in items:
                pub_date_str = item['pubDate']
                try:
                    pub_date = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %z")
                    if category_tag == 'market':
                        now = datetime.now(pub_date.tzinfo)
                        if (now - pub_date) > timedelta(hours=12):
                            continue
                except:
                    pass

                raw_title = item['title']
                clean_title = html.unescape(raw_title).replace("<b>", "").replace("</b>", "")
                
                raw_desc = item['description']
                clean_desc = html.unescape(raw_desc).replace("<b>", "").replace("</b>", "")
                
                link = item['originallink'] if item['originallink'] else item['link']

                if any(ex_word in clean_title for ex_word in excludes): continue
                if not any(key_word in clean_title for key_word in target_keywords): continue
                
                results.append({'title': clean_title, 'url': link, 'desc': clean_desc, 'category': category_tag})
            time.sleep(0.3) 
        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬: {e}")
            break
            
    return results

def remove_duplicates_globally(all_news):
    unique_news = []
    seen_urls = set()
    seen_descriptions = []

    print("ğŸ§¹ ì¤‘ë³µ ì œê±° ì¤‘...")

    for item in all_news:
        if item['url'] in seen_urls: continue
            
        category = item.get('category', 'general')
        threshold = 60 if category == 'market' else 12
            
        is_content_dup = False
        for exist_desc in seen_descriptions:
            matcher = difflib.SequenceMatcher(None, item['desc'], exist_desc)
            if matcher.find_longest_match(0, len(item['desc']), 0, len(exist_desc)).size >= threshold: 
                is_content_dup = True
                break
        
        if is_content_dup: continue

        seen_urls.add(item['url'])
        seen_descriptions.append(item['desc'])
        unique_news.append(item)

    return unique_news

# ==========================================
# ğŸ› ï¸ ìˆ˜ì •ëœ ë¶€ë¶„: íŠ¹ìˆ˜ë¬¸ì(<, >) ì²˜ë¦¬
# ==========================================
def format_news_report(news_data):
    sector_invest = []   
    sector_industry = [] 

    for item in news_data:
        title = item['title']
        # ê¸°ì‚¬ ì œëª©ì˜ <, > ë„ ì•ˆì „í•˜ê²Œ ë³€í™˜
        safe_title = html.escape(title)
        item['safe_title'] = safe_title
        
        invest_keywords = ['ì†ìµ', 'ì‹¤ì ', 'íˆ¬ì', 'IR', 'ë‰´ìš•ì¦ì‹œ', 'ì½”ìŠ¤í”¼', 'ë§ˆê°', 'ì‹œí™©', 'ì£¼ê°€', 'ì¦ì‹œ']
        
        if any(k in title for k in invest_keywords):
            sector_invest.append(item)
        else:
            sector_industry.append(item)
    
    now = datetime.now()
    days_kr = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
    today_str = f"{now.strftime('%Y.%m.%d')}({days_kr[now.weekday()]})"
    
    report = f"<b>â–  News feed: {today_str}</b>\n\n"
    
    # ğŸš¨ ì—¬ê¸°ê°€ ë¬¸ì œì˜€ìŒ! < > ë¥¼ &lt; &gt; ë¡œ ë³€ê²½
    report += "<b>&lt;ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„&gt;</b>\n" 
    if not sector_industry: report += "(ê¸°ì‚¬ ì—†ìŒ)\n"
    for item in sector_industry:
        report += f"â€¢ <a href='{item['url']}'>{item['safe_title']}</a>\n"
        
    # ğŸš¨ ì—¬ê¸°ë„ ë³€ê²½
    report += "\n<b>&lt;íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥&gt;</b>\n"
    if not sector_invest: report += "(ê¸°ì‚¬ ì—†ìŒ)\n"
    for item in sector_invest:
        report += f"â€¢ <a href='{item['url']}'>{item['safe_title']}</a>\n"
        
    return report

def send_telegram(message):
    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    
    if not token or not chat_id:
        print("ğŸ”” í…”ë ˆê·¸ë¨ ì„¤ì • ì—†ìŒ")
        return

    try:
        url = f"https://api.telegram.org/bot{token}/sendMessage"
        data = {
            'chat_id': chat_id, 
            'text': message, 
            'parse_mode': 'HTML',
            'disable_web_page_preview': True
        }
        response = requests.post(url, data=data)
        
        if response.status_code == 200:
            print("ğŸš€ í…”ë ˆê·¸ë¨ ì „ì†¡ ì™„ë£Œ")
        else:
            print(f"âŒ ì „ì†¡ ì‹¤íŒ¨ (Code: {response.status_code})")
            print(f"ğŸ‘‰ ì›ì¸: {response.text}")
    except Exception as e:
        print(f"í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    should_skip, reason = is_skip_day()
    if should_skip:
        print(f"â›” ì˜¤ëŠ˜ì€ '{reason}'ì´ë¯€ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit()

    KEYWORDS_INSURANCE = ["ì‚¼ì„±ìƒëª…", "í•œí™”ìƒëª…", "êµë³´ìƒëª…", "ìƒë³´ì‚¬", "ë³´í—˜ì‚¬"]
    KEYWORDS_MARKET = ["ë§ˆê°ì‹œí™©", "ë§ˆê° ì‹œí™©", "ë‰´ìš•ì¦ì‹œ","ì½”ìŠ¤í”¼","FOMC","ê¸ˆí†µìœ„","í•œì€"] 
    EXCLUDES = ["ë¶€ê³ ", "ë°°íƒ€ì ", "ìƒí’ˆ", "ê°„ë³‘", "ì‚¬ì—…ë¹„", "ë³´í—˜ê¸ˆ", "ì—°ê¸ˆë³´í—˜", "ë¯¼ì›", "ì¶œì‹œ", "ì†í•´ì‚¬ì •",
                "ì±„ë„ ê²½ìŸ", "ë¹„ê¸‰ì—¬", "ì›ë¦¬ê¸ˆ", "ë³´ì¥í˜•", "IRP", "ì¦ì—¬", "ìë™ì°¨", "íŠ¹ì•½", "ìœ¤ë¦¬","ì†Œë¹„ì"]
    
    if "YOUR_CLIENT_ID" in NAVER_CLIENT_ID: 
         print("âš ï¸ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    else:
        news_insurance = crawl_naver_news_api(KEYWORDS_INSURANCE, excludes=EXCLUDES, display_limit=60, category_tag='insurance')
        news_market = crawl_naver_news_api(KEYWORDS_MARKET, excludes=[], display_limit=20, category_tag='market')
        news_market = news_market[:3] 

        final_list = remove_duplicates_globally(news_insurance + news_market)
        final_msg = format_news_report(final_list)
        
        print("-" * 30)
        print(final_msg)
        print("-" * 30)
        
        send_telegram(final_msg)
        
