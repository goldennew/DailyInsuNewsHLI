import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import os

# --- 1. ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜ (ê°•í™”í˜•) ---
def crawl_naver_news(base_keywords, include_words=None, exclude_words=None, pages=5):
    query_parts = []
    if isinstance(base_keywords, str): base_keywords = [base_keywords]
    # ë„¤ì´ë²„ ê²€ìƒ‰ ì—°ì‚°ìì— ë§ê²Œ ì¿¼ë¦¬ ì¬êµ¬ì„±
    query = " ".join(base_keywords) 
    if include_words: query += " " + " ".join([f"+{word}" for word in include_words])
    if exclude_words: query += " " + " ".join([f"-{word}" for word in exclude_words])
    
    base_url = "https://search.naver.com/search.naver"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Referer': 'https://www.naver.com'
    }

    results = []
    print(f"ğŸ” ê²€ìƒ‰ì–´ [{query}]ë¡œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    for page in range(pages):
        start_val = (page * 10) + 1
        params = {
            'where': 'news',
            'query': query,
            'sm': 'tab_opt',
            'sort': '1',        # ìµœì‹ ìˆœ
            'pd': '2',          # 48ì‹œê°„ ì´ë‚´
            'start': start_val
        }
        
        try:
            response = requests.get(base_url, headers=headers, params=params, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ì˜ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ ì„ íƒì ëŒ€ì‘
            articles = soup.select("div.news_wrap.api_ani_send")
            if not articles:
                articles = soup.select("ul.list_news > li") # ëŒ€ì²´ ì„ íƒì

            found_count = 0
            for article in articles:
                title_tag = article.select_one("a.news_tit")
                if not title_tag: continue
                
                text = title_tag.get_text().strip()
                href = title_tag.get('href')

                # í•„í„°ë§ ë¡œì§
                # 1. ì œëª© ê¸¸ì´ (10~200ì)
                if not (10 <= len(text) <= 200): continue
                # 2. ì œì™¸ í‚¤ì›Œë“œ
                if exclude_words and any(bad in text for bad in exclude_words): continue
                # 3. ì¤‘ë³µ ì œê±°
                if any(r['url'] == href for r in results): continue

                results.append({'title': text, 'url': href})
                found_count += 1
            
            print(f"ğŸ“„ {page+1}í˜ì´ì§€ì—ì„œ {found_count}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            if found_count == 0: break # ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
            
            time.sleep(0.8) # ì°¨ë‹¨ ë°©ì§€ìš© ì§€ì—°
        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")
            break
            
    return results

# --- 2. ë°ì´í„° ë¶„ë¥˜ ë° í˜•ì‹í™” (ë™ì¼) ---
def format_news_feed(news_data):
    sector_investment = [] # <íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>
    sector_industry = []   # <ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>

    for item in news_data:
        # ë¶„ë¥˜ í‚¤ì›Œë“œ
        if any(word in item['title'] for word in ['ì†ìµ', 'ìì‚°', 'íˆ¬ì', 'ì¦ì‹œ', 'ê¸ˆë¦¬']):
            if len(sector_investment) < 5:
                sector_investment.append(item)
        else:
            if len(sector_industry) < 5:
                sector_industry.append(item)
        
        if len(sector_investment) >= 5 and len(sector_industry) >= 5:
            break

    today_str = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    output = f"â– News feed: {today_str}\n\n"
    
    output += "<ìƒë³´3ì‚¬/ë³´í—˜ì—…ê³„>\n\n"
    if not sector_industry:
        output += "(ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤)\n\n"
    for a in sector_industry:
        output += f"{a['title']}\n{a['url']}\n\n"
    
    output += "<íˆ¬ìì†ìµ/ê¸ˆìœµì‹œì¥>\n\n"
    if not sector_investment:
        output += "(ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤)\n\n"
    for a in sector_investment:
        output += f"{a['title']}\n{a['url']}\n\n"
    
    return output

# --- 3. í…”ë ˆê·¸ë¨ ì „ì†¡ (ë™ì¼) ---
def send_telegram_message(message):
    token = os.environ.get('TELEGRAM_BOT_TOKEN')
    chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    if not token or not chat_id: return

    url = f"https://api.telegram.org/bot{token}/sendMessage"
    # ë§í¬ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ disable_web_page_preview ì˜µì…˜ì€ ì·¨í–¥ê» ì¡°ì ˆ ê°€ëŠ¥
    payload = {'chat_id': chat_id, 'text': message, 'disable_web_page_preview': False}
    requests.post(url, data=payload)

# --- 4. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    KEYWORDS = ["í•œí™”ìƒëª…", "ì‚¼ì„±ìƒëª…", "êµë³´ìƒëª…", "ë³´í—˜ì‚¬"]
    EXCLUDES = ["ë°°íƒ€ì ", "ë¯¼ì›", "ì§€ì‹iN", "ê³ ê°ì„¼í„°"]
    
    raw_news = crawl_naver_news(KEYWORDS, exclude_words=EXCLUDES, pages=5)
    final_report = format_news_feed(raw_news)
    
    print(final_report)
    send_telegram_message(final_report)
