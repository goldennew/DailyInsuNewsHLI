import requests
from bs4 import BeautifulSoup
import time
import os
from datetime import datetime
import random
from urllib.parse import quote  # í•œê¸€ ì¸ì½”ë”©ì„ ìœ„í•´ ì¶”ê°€

def crawl_naver_news_robust(keywords, pages=2):
    session = requests.Session()
    
    # ìµœì‹  ëª¨ë°”ì¼ ë¸Œë¼ìš°ì € í—¤ë”
    headers = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 13; SM-S918B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Mobile Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
    }

    try:
        session.get("https://m.naver.com", headers=headers, timeout=10)
    except:
        pass

    results = []
    query = " ".join(keywords)
    # í•œê¸€ ê²€ìƒ‰ì–´ë¥¼ URL ì•ˆì „í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    encoded_query = quote(query)
    
    print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: {query}")

    base_url = "https://m.search.naver.com/search.naver"

    for page in range(pages):
        start = (page * 15) + 1
        params = {
            'where': 'm_news',
            'query': query, # paramsì— ë„£ì„ ë•ŒëŠ” requestsê°€ ì•Œì•„ì„œ ì¸ì½”ë”©í•˜ì§€ë§Œ
            'sm': 'mtb_pge',
            'sort': '1',
            'nso': 'so:dd,p:2d',
            'start': start
        }

        # í—¤ë”ì˜ Refererì—ëŠ” ì§ì ‘ ì¸ì½”ë”©ëœ ë¬¸ìì—´ì„ ë„£ì–´ì¤˜ì•¼ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        headers['Referer'] = f"https://m.search.naver.com/search.naver?where=m_news&query={encoded_query}"

        try:
            time.sleep(random.uniform(1.5, 2.5))
            
            # ì—¬ê¸°ì„œ headersì— ì¸ì½”ë”©ëœ Refererê°€ í¬í•¨ë˜ì–´ ì—ëŸ¬ë¥¼ ë°©ì§€í•¨
            response = session.get(base_url, headers=headers, params=params, timeout=15)
            
            if response.status_code != 200:
                print(f"âŒ ì ‘ì† ì‹¤íŒ¨ (ìƒíƒœì½”ë“œ: {response.status_code})")
                continue

            if "ë¡œë´‡" in response.text or "CAPTCHA" in response.text:
                print("ğŸš¨ ë„¤ì´ë²„ ì°¨ë‹¨ ê°ì§€: IPê°€ ì œí•œë˜ì—ˆê±°ë‚˜ ë´‡ìœ¼ë¡œ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break

            soup = BeautifulSoup(response.text, 'html.parser')
            news_items = soup.select("li.bx") 

            found_in_page = 0
            for item in news_items:
                title_tag = item.select_one("a.news_tit")
                if not title_tag: continue
                
                text = title_tag.get_text().strip()
                href = title_tag.get('href')

                if 10 < len(text) < 100 and href.startswith("http"):
                    if any(r['url'] == href for r in results): continue
                    results.append({'title': text, 'url': href})
                    found_in_page += 1

            print(f"ğŸ“„ {page+1}í˜ì´ì§€: {found_in_page}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            if found_in_page == 0: break

        except Exception as e:
            print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")
            break

    return results

# ... (ë‚˜ë¨¸ì§€ í•¨ìˆ˜ëŠ” ë™ì¼)
